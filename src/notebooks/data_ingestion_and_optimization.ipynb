{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daa37cd0-141d-4e88-a41d-1bb77174c48f",
   "metadata": {},
   "source": [
    "# Task 2: Data Ingestion & Optimization\n",
    "###### 1. Read all raw CSV files from HDFS.\n",
    "###### 2. Apply proper schema instead of inferSchema.\n",
    "###### 3. Handle null values.\n",
    "###### 4. Convert raw CSV files into Parquet format.\n",
    "###### 5. Store them in /data/covid/staging.\n",
    "##### Compare CSV vs Parquet:\n",
    "###### 1. File size\n",
    "###### 2. Read performance\n",
    "###### 3. Execution plan\n",
    "###### Explain why Parquet performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cad3d2c7-1dc0-45b8-998c-252f2a55ca21",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Statements\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "531b5313-0cd3-4183-a452-17d4fb344812",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Starting Session\n",
    "spark = SparkSession.builder.appName(\"Data Ingestion\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f123a18-b37d-49ee-a592-400e1b174d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.118.16.230:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Data Ingestion</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x111a2f620>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90047188-e129-4536-9c36-b2da133ea10f",
   "metadata": {},
   "source": [
    "### Covid_19_clean_complete Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0f8ef4d-b9d5-4804-8394-b829c537fe44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "#### schema for covid_19_clean_complete table\n",
    "covid_clean_schema = StructType([\n",
    "    StructField(\"Province/State\", StringType(), True),\n",
    "    StructField(\"Country/Region\", StringType(), True),\n",
    "    StructField(\"Lat\", DoubleType(), True),\n",
    "    StructField(\"Long\", DoubleType(), True),\n",
    "    StructField(\"Date\", DateType(), True),\n",
    "    StructField(\"Confirmed\", LongType(), True),\n",
    "    StructField(\"Deaths\", LongType(), True),\n",
    "    StructField(\"Recovered\", LongType(), True),\n",
    "    StructField(\"Active\", LongType(), True),\n",
    "    StructField(\"WHO Region\", StringType(), True)\n",
    "])\n",
    "\n",
    "#### Loading data using defined schema.\n",
    "covid_clean = spark.read.option(\"header\",True).schema(covid_clean_schema).csv(\"hdfs:///data/covid/raw/covid_19_clean_complete.csv\")\n",
    "\n",
    "#### Handling null values.\n",
    "covid_clean = covid_clean.na.drop(subset=[\"Date\", \"Country/Region\"])\n",
    "covid_clean = covid_clean.na.fill({\n",
    "    \"Province/State\": \"Unknown\",\n",
    "    \"Lat\": 0.0,\n",
    "    \"Long\": 0.0,\n",
    "    \"Confirmed\": 0,\n",
    "    \"Deaths\": 0,\n",
    "    \"Recovered\": 0,\n",
    "    \"Active\": 0,\n",
    "    \"WHO Region\": \"Unknown\"\n",
    "})\n",
    "\n",
    "#### Converting raw data into paraquet format.\n",
    "covid_clean.write.mode(\"overwrite\").parquet(\"hdfs:///data/covid/staging/covid_clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49ccdcc-ef93-4e54-852d-55d45821835f",
   "metadata": {},
   "source": [
    "### Full_grouped Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f3e8733-4650-4d0d-974f-8b994928f038",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "#### Schema for full_grouped table\n",
    "full_grouped_schema = StructType([\n",
    "    StructField(\"Date\", DateType(), True),\n",
    "    StructField(\"Country/Region\", StringType(), True),\n",
    "    StructField(\"Confirmed\", LongType(), True),\n",
    "    StructField(\"Deaths\", LongType(), True),\n",
    "    StructField(\"Recovered\", LongType(), True),\n",
    "    StructField(\"Active\", LongType(), True),\n",
    "    StructField(\"New cases\", LongType(), True),\n",
    "    StructField(\"New deaths\", LongType(), True),\n",
    "    StructField(\"New recovered\", LongType(), True),\n",
    "    StructField(\"WHO Region\", StringType(), True)\n",
    "])\n",
    "\n",
    "#### Loading data using defined schema.\n",
    "full_grouped = spark.read.option(\"header\",True).schema(full_grouped_schema).csv(\"hdfs:///data/covid/raw/full_grouped.csv\")\n",
    "\n",
    "#### Handling null values.\n",
    "full_grouped = full_grouped.na.drop(subset=[\"Date\", \"Country/Region\"])\n",
    "full_grouped = full_grouped.na.fill({\n",
    "    \"Confirmed\": 0,\n",
    "    \"Deaths\": 0,\n",
    "    \"Recovered\": 0,\n",
    "    \"Active\": 0,\n",
    "    \"New cases\": 0,\n",
    "    \"New deaths\": 0,\n",
    "    \"New recovered\": 0,\n",
    "    \"WHO Region\": \"Unknown\"\n",
    "})\n",
    "\n",
    "#### Converting raw data into paraquet format.\n",
    "full_grouped.write.mode(\"overwrite\").parquet(\"hdfs:///data/covid/staging/full_grouped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed2d987-4c7d-4af1-9b90-eab6c9913f7d",
   "metadata": {},
   "source": [
    "### Country_Wise_Latest Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b24e6aee-e320-45f8-985a-68e0e7e5259d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/17 19:57:35 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 15, schema size: 10\n",
      "CSV file: hdfs://localhost:9000/data/covid/raw/country_wise_latest.csv\n"
     ]
    }
   ],
   "source": [
    "#### Schema for country_wise_latest table\n",
    "country_wise_latest_schema = StructType([\n",
    "    StructField(\"Country/Region\", StringType(), True),\n",
    "    StructField(\"Confirmed\", LongType(), True),\n",
    "    StructField(\"Deaths\", LongType(), True),\n",
    "    StructField(\"Recovered\", LongType(), True),\n",
    "    StructField(\"Active\", LongType(), True),\n",
    "    StructField(\"New cases\", LongType(), True),\n",
    "    StructField(\"New deaths\", LongType(), True),\n",
    "    StructField(\"New recovered\", LongType(), True),\n",
    "    StructField(\"Deaths / 100 Cases\", DoubleType(), True),\n",
    "    StructField(\"Recovered / 100 Cases\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "#### Loading data using defined schema.\n",
    "country_wise_latest = spark.read.option(\"header\",True).schema(country_wise_latest_schema).csv(\"hdfs:///data/covid/raw/country_wise_latest.csv\")\n",
    "\n",
    "#### Handling null values.\n",
    "country_wise_latest = country_wise_latest.na.drop(subset=[\"Country/Region\"])\n",
    "country_wise_latest = country_wise_latest.na.fill({\n",
    "    \"Confirmed\": 0,\n",
    "    \"Deaths\": 0,\n",
    "    \"Recovered\": 0,\n",
    "    \"Active\": 0,\n",
    "    \"New cases\": 0,\n",
    "    \"New deaths\": 0,\n",
    "    \"New recovered\": 0,\n",
    "    \"Deaths / 100 Cases\": 0.0,\n",
    "    \"Recovered / 100 Cases\": 0.0\n",
    "})\n",
    "\n",
    "#### Converting raw data into paraquet format.\n",
    "country_wise_latest.write.mode(\"overwrite\").parquet(\"hdfs:///data/covid/staging/country_wise_latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b4441e-a67f-407b-93be-6d42a4d8d20d",
   "metadata": {},
   "source": [
    "### Day_Wise Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b03fe7b7-2ab5-490b-93fd-1bde92fbb2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/17 19:57:36 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 12, schema size: 10\n",
      "CSV file: hdfs://localhost:9000/data/covid/raw/day_wise.csv\n"
     ]
    }
   ],
   "source": [
    "#### Schema for day_wise table\n",
    "day_wise_schema = StructType([\n",
    "    StructField(\"Date\", DateType(), True),\n",
    "    StructField(\"Confirmed\", LongType(), True),\n",
    "    StructField(\"Deaths\", LongType(), True),\n",
    "    StructField(\"Recovered\", LongType(), True),\n",
    "    StructField(\"Active\", LongType(), True),\n",
    "    StructField(\"New cases\", LongType(), True),\n",
    "    StructField(\"New deaths\", LongType(), True),\n",
    "    StructField(\"New recovered\", LongType(), True),\n",
    "    StructField(\"Deaths / 100 Cases\", DoubleType(), True),\n",
    "    StructField(\"Recovered / 100 Cases\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "#### Loading data using defined schema.\n",
    "day_wise = spark.read.option(\"header\",True).schema(day_wise_schema).csv(\"hdfs:///data/covid/raw/day_wise.csv\")\n",
    "\n",
    "#### Handling null values.\n",
    "day_wise = day_wise.na.drop(subset=[\"Date\"])\n",
    "\n",
    "day_wise = day_wise.na.fill({\n",
    "    \"Confirmed\": 0,\n",
    "    \"Deaths\": 0,\n",
    "    \"Recovered\": 0,\n",
    "    \"Active\": 0,\n",
    "    \"New cases\": 0,\n",
    "    \"New deaths\": 0,\n",
    "    \"New recovered\": 0,\n",
    "    \"Deaths / 100 Cases\": 0.0,\n",
    "    \"Recovered / 100 Cases\": 0.0\n",
    "})\n",
    "\n",
    "\n",
    "#### Converting raw data into paraquet format.\n",
    "day_wise.write.mode(\"overwrite\").parquet(\"hdfs:///data/covid/staging/day_wise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc89ceba-f6f5-4527-ba98-96be67bbcfa5",
   "metadata": {},
   "source": [
    "### USA_Country_Wise Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c11c520-bc65-4aeb-aa05-5fb9f5e1fa6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/17 19:57:36 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 14, schema size: 10\n",
      "CSV file: hdfs://localhost:9000/data/covid/raw/usa_county_wise.csv\n",
      "26/02/17 19:57:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "#### Schema for usa_country_wise table\n",
    "usa_county_wise_schema = StructType([\n",
    "    StructField(\"UID\", LongType(), True),\n",
    "    StructField(\"iso2\", StringType(), True),\n",
    "    StructField(\"iso3\", StringType(), True),\n",
    "    StructField(\"code3\", IntegerType(), True),\n",
    "    StructField(\"FIPS\", DoubleType(), True),\n",
    "    StructField(\"Admin2\", StringType(), True),\n",
    "    StructField(\"Province_State\", StringType(), True),\n",
    "    StructField(\"Country_Region\", StringType(), True),\n",
    "    StructField(\"Lat\", DoubleType(), True),\n",
    "    StructField(\"Long_\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "#### Loading data using defined schema.\n",
    "usa_country_wise = spark.read.option(\"header\",True).schema(usa_county_wise_schema).csv(\"hdfs:///data/covid/raw/usa_county_wise.csv\")\n",
    "\n",
    "#### Handling null values.\n",
    "usa_country_wise = usa_country_wise.na.drop(subset=[\"UID\", \"Province_State\"])\n",
    "usa_country_wise = usa_country_wise.na.fill({\n",
    "    \"iso2\": \"Unknown\",\n",
    "    \"iso3\": \"Unknown\",\n",
    "    \"Admin2\": \"Unknown\",\n",
    "    \"Country_Region\": \"Unknown\",\n",
    "    \"Lat\": 0.0,\n",
    "    \"Long_\": 0.0\n",
    "})\n",
    "\n",
    "#### Converting raw data into paraquet format.\n",
    "usa_country_wise.write.mode(\"overwrite\").parquet(\"hdfs:///data/covid/staging/usa_country_wise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49963235-d85f-4503-bb36-1284fe835e79",
   "metadata": {},
   "source": [
    "### Worldometer_data Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fc3e451-bad1-4fc2-ada6-ce6fa0ec7ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/17 19:57:38 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 16, schema size: 10\n",
      "CSV file: hdfs://localhost:9000/data/covid/raw/worldometer_data.csv\n"
     ]
    }
   ],
   "source": [
    "#### Schema for worldometer_data table\n",
    "worldometer_data_schema = StructType([\n",
    "    StructField(\"Country/Region\", StringType(), True),\n",
    "    StructField(\"Continent\", StringType(), True),\n",
    "    StructField(\"Population\", LongType(), True),\n",
    "    StructField(\"TotalCases\", LongType(), True),\n",
    "    StructField(\"NewCases\", LongType(), True),\n",
    "    StructField(\"TotalDeaths\", LongType(), True),\n",
    "    StructField(\"NewDeaths\", LongType(), True),\n",
    "    StructField(\"TotalRecovered\", LongType(), True),\n",
    "    StructField(\"NewRecovered\", LongType(), True),\n",
    "    StructField(\"ActiveCases\", LongType(), True)\n",
    "])\n",
    "\n",
    "## Loading data using defined schema.\n",
    "worldometer_data = spark.read.option(\"header\",True).schema(worldometer_data_schema).csv(\"hdfs:///data/covid/raw/worldometer_data.csv\")\n",
    "\n",
    "#### Handling null values.\n",
    "worldometer_data = worldometer_data.na.drop(subset=[\"Country/Region\", \"Population\"])\n",
    "worldometer_data = worldometer_data.na.fill({\n",
    "    \"Continent\": \"Unknown\",\n",
    "    \"TotalCases\": 0,\n",
    "    \"NewCases\": 0,\n",
    "    \"TotalDeaths\": 0,\n",
    "    \"NewDeaths\": 0,\n",
    "    \"TotalRecovered\": 0,\n",
    "    \"NewRecovered\": 0,\n",
    "    \"ActiveCases\": 0\n",
    "})\n",
    "\n",
    "#### Converting raw data into paraquet format.\n",
    "worldometer_data.write.mode(\"overwrite\").parquet(\"hdfs:///data/covid/staging/worldometer_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4801de1d-a63d-48ea-8b36-5c323de98766",
   "metadata": {},
   "source": [
    "### Comparing File sizes of Hadoop and Paraquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af127d95-f233-455b-9dfd-a391ad0fdf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size in Hadoop...\n",
      "2026-02-17 19:57:39,520 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "14.4 K  14.4 K  /data/covid/raw/country_wise_latest.csv\n",
      "3.2 M   3.2 M   /data/covid/raw/covid_19_clean_complete.csv\n",
      "14.1 K  14.1 K  /data/covid/raw/day_wise.csv\n",
      "1.8 M   1.8 M   /data/covid/raw/full_grouped.csv\n",
      "66.6 M  66.6 M  /data/covid/raw/usa_county_wise.csv\n",
      "16.1 K  16.1 K  /data/covid/raw/worldometer_data.csv\n",
      "\n",
      "File size in paraquet...\n",
      "2026-02-17 19:57:40,587 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "11.5 K   11.5 K   /data/covid/staging/country_wise_latest\n",
      "381.4 K  381.4 K  /data/covid/staging/covid_clean\n",
      "11.8 K   11.8 K   /data/covid/staging/day_wise\n",
      "417.9 K  417.9 K  /data/covid/staging/full_grouped\n",
      "2.9 M    2.9 M    /data/covid/staging/usa_country_wise\n",
      "10.1 K   10.1 K   /data/covid/staging/worldometer_data\n"
     ]
    }
   ],
   "source": [
    "## Comparing \n",
    "print(\"File size in Hadoop...\")\n",
    "!hdfs dfs -du -h /data/covid/raw\n",
    "print(\"\\nFile size in paraquet...\")\n",
    "!hdfs dfs -du -h /data/covid/staging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3798599-2fdf-47b4-b170-7ce049eee0c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "391278b2-d746-488d-995c-fb0f8406691a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6530e6b1-7687-4fa3-b3d2-9d1933350540",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
